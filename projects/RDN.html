
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--suppress CheckImageSize -->
<html xmlns="http://www.w3.org/1999/xhtml">

  <head>

    <script src="js/head.js?prefix="></script>
    <meta name="description" content="Vision+Image Captioning+RDN">
    <meta name="keywords" content="HKUST, Vision, Image Captioning, RDN, ICCV">

    <title>Reflective Decoding Network for Image Captioning</title>
    <link rel="stylesheet" href="./font.css">
    <link rel="stylesheet" href="./main.css">

  </head>

  <body>

    <div class="outercontainer">
      <div class="container">

        <div class="content project_title">
          <h1>Reflective Decoding Network for Image Captioning</h1>
          <div class="authors">
                  <a href="https://www.kelei.site">Lei Ke<sup>1</sup></a>,
                  <a href="https://wenjiepei.github.io">Wenjie Pei<sup>2</sup></a>,
                  Ruiyu Li<sup>2</sup>,
                  <a href="https://xiaoyongshen.me">Xiaoyong Shen<sup>2</sup></a>, and
                  <a href="https://scholar.google.com/citations?user=nFhLmFkAAAAJ&hl=en">Yu-Wing Tai<sup>2</sup></a>
        </div>
                  <div class="authors">
                  <sup>1</sup><a href="https://www.ust.hk/"><i>The Hong Kong University of Science and Technology</i></a>,
                  <sup>2</sup><i>Tencent Youtu Lab</i></a>
        </div>
        <div>
                  <span class="venue"><a href="http://iccv2019.thecvf.com/">ICCV 2019</a></span>
                  <span class="tag"><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Ke_Reflective_Decoding_Network_for_Image_Captioning_ICCV_2019_paper.pdf">Paper</a></span>
                  <span class="tag"><a href="https://arxiv.org/pdf/1908.11824.pdf">arXiv</a></span>
                  <span class="tag"><a href="../poster/poster_RDNCaption.pdf">Poster</a></span>
                  <span class="tag"><a href="http://openaccess.thecvf.com/content_ICCV_2019/supplemental/Ke_Reflective_Decoding_Network_ICCV_2019_supplemental.pdf">Supp</a></span>
                  <span class="tag"><a href="bibtex/rdn.bib">BibTeX</a></span>
                  <span class="tag"><a href="#split">HardImageCaptioningSplit</a></span>

                </div>
        </div>
        <div class="content project_headline">
          <div class="img" style="text-align:center">
            <img class="img_responsive" src="images/framework.png" alt="Model" style="margin:auto;max-width:80%"/>
          </div>
          <div class="text">
            <p><i>Figure 1: Overview of our Reflective Decoding Network (RDN) for Image Captioning</i></p>
          </div>
        </div>
        <div class="content project_headline">
          <div class="img" style="text-align:center">
            <img class="img_responsive" src="images/samples.png" alt="samples" style="margin:auto;max-width:80%"/>
          </div>
          <div class="text">
            <p><i>Figure 2: Examples of captions generated by our RDN compared to the basis decoder (using traditional LSTM) and their reflective attention
weight distribution over the past generated hidden states when predicting the key words highlighted in green. The thicker line indicates a
relatively larger weight and the red line means the largest contribution.</i></p>
          </div>
        </div>

        <div class="content">
          <div class="text">
            <h2 style='text-align: center;'>Abstract</h2>
            <p>State-of-the-art image captioning methods mostly focus on improving visual features, less attention has been paid to <b><u>utilizing the inherent properties of language to boost captioning performance</u></b>. In this paper, we show that vocabulary coherence between words and syntactic paradigm of sentences are also important to generate high-quality image caption. Following the conventional encoder-decoder framework, we propose the Reflective Decoding Network (RDN) for image captioning, which enhances both the long-sequence dependency and position perception of words in a caption decoder. Our model learns to collaboratively attend on both visual and textual features and meanwhile perceive each word's relative position in the sentence to maximize the information delivered in the generated caption. We evaluate the effectiveness of our RDN on the COCO image captioning datasets and achieve superior performance over the previous methods. Further experiments reveal that our approach is particularly advantageous for <b><u>hard cases with complex scenes to describe by captions</u></b>.</p>
          </div>
        </div>

        <div class="content">
          <div class="text">
            <h2 id='split' style='text-align: center;'>Hard Image Captioning</h2>
            <div>
                  <span class="venue">Download:</span>
                  <span class="tag"><a href="https://drive.google.com/drive/folders/1zeyOWnuHl9zTwpxgkWxjIZJwkYQ1tKce">Hard Captioning Split set (Image IDs)</a></span>
                  <span class="tag"><a href="./images/CIDEr score.htm">CIDEr Scores for Comparison on Hard Split</a></span>
                </div>
            <p><b>Definition:</b> Compared to traditional captioning performance evaluation on 'Karpathy' splits, in this paper, we further investigate the effect of the average length of annotations (ground truth captions) on the captioning performance, since generally the images with averagely longer annotations contain more complex scenes and thus are harder for captioning. Specifically, <b><u>we rank the whole ‘Karparthy’ testset (5000 images) according to their average length of annotations in descending order and extract four different size of subsets (all set, top-1000, top-500, top-300 respectively)</u></b>. Smaller subset corresponds to averagely longer annotations and implies harder image captioning.</p>
                    <div class="content project_headline">
          <div class="img" style="text-align:center">
            <img class="img_responsive" src="images/hard.png" alt="hard" style="margin:auto;max-width:80%"/>
          </div>
          <div class="text">
            <p><i>Figure 3: Performance comparison between our RDN model and Up-Down on hard Image Captioning as a function of average length of annotations (ground truth captions). We rank the ‘Karpathy’ test set according to their average length of annotations in descending order and extract four different size of subsets. Smaller subset corresponds to averagely longer annotations and harder captioning. It reveals that our model exhibits more superiority over Up-Down in harder cases.</i></p>
          </div>
        </div>
            <p> Figure 3 shows the comparison between our RDN and Up-Down (main difference of the two models is that Up-Down uses traditional LSTM). It reveals that the performance of both models are decreasing with the increasing average length of annotations, which reflects that the captioning is getting harder. However, our model exhibits more superiority over Up-Down in harder cases, which in turn validates the ability of our RDN to capture the long-term dependencies within captions.</p>
          </div>
          </div>
        </div>
 
      </div>
    </div>

  </body>

</html>
